\section{Implementation}
For our classifier, we built a deep convolutional neural network. We chose to base our architecture on AlexNet, as we saw in lecture that this architecture has been studied thoroughly in the research literature and has proven effective in a variety of contexts. All of our layers have the same architecture as the original network except for the final fully connected layer, which is modified so that the network produces the same number of output classes. In total, our network has eight layers.

Inspired by the transfer learning results presented in lecture, we initialized the first four layers of the network with the AlexNet weights. Training took place in two stages: in the first stage, we trained the weights in the second four layers of the network. In the second stage, we retrained the AlexNet weights in the first four layers. We expected a two-stage approach to produce more robust weights that would generalize better. In addition, we expected that the first few layers to capture features essential to vision and classification in general, while we expected the layers in the second half of the network to capture features specific to the landmark recognition problem that we are solving. We chose to retrain the original AlexNet weights from the first four layers because the graphs presented in the transfer learning lecture suggested that there can be an improvement accuracy following further training of transferred weights.

We believe our problem to be similar to the ImageNet challenge, so we felt that it would be inappropriate to make large modifications to the AlexNet network topology that has proven to be highly effective.

To tune hyperparameters such as the learning rate for the novel layers and the original AlexNet layers, we trained the model on a small subset of the data. We achieved acceptable performance with a learning rate of 0.001 for the novel layers and 0.0002 for the original AlexNet layers.

In our tests, we found that a L2 regularizer was not particularly helpful, so we did not think it necessary to include one in our code. We think that our network architecture, which has a number of shared parameters in the convolutional layers, served as a form of regularization.